{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<h1 align=\"center\">Python Programming for Machine Learning</h1>\n",
    "<h2 align=\"center\">Acceleration Frameworks ‚Äì Part 2</h2>\n",
    "\n",
    "<center><img src='images/python-logo-only.svg' width=250> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NumPy version: 1.26.4\n",
      "PyTorch version: 2.5.1\n",
      "Matplotlib version: 3.9.2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Matplotlib version: {matplotlib.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reminder:\n",
    "- Lecturer: Have you started the recording?\n",
    "- Audience: Have you received a notification that the recording has started?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensor ops: bigger picture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section:\n",
    "- partially exam-relevant, e.g., the exam will **not** ask qualitative questions about JIT\n",
    "- helps with coding tensor ops by understaning their context & helps with projects outside of PyML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to implement fast code \"from first principles\"?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bigger picture\n",
    "\n",
    "Abstractions:\n",
    "- Implementation: max runtime performance\n",
    "- Interface: max development speed (readability, ecosystem)\n",
    "\n",
    "Python advantages:\n",
    "- Interface is clean **and** adaptable (e.g., libraries can overload operators like `+`): readable code **and** speed\n",
    "- Mature ecosystem: user can benefit from interface without knowing C/CPP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor ops: abstraction layers\n",
    "\n",
    "- GPU: machine code / assembly-like / PTX / ...\n",
    "- Kernel/compiler/vendor libs: e.g., CUDA / CUBLAS / CUDNN / XLA / ...\n",
    "- Python interface: torch / JAX / ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding tensor ops without the \"frontend\"\n",
    "\n",
    "Frontend: e.g., torch.matmul vs. einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üéØ Goal: attain general skills, don't overfit on torch syntax.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[tinygrad](https://tinygrad.org/#tinygrad):\n",
    "- Autograd enginge (similar to torch)\n",
    "- Pro: can run on GPUs of many vendors\n",
    "- Con: less mature, less adoption, less speed than specialized frameworks\n",
    "- Method:\n",
    "  - Compiler/abstraction over vendor-specific kernels\n",
    "  - **Requires low complexity** (only three core ops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üí° We can express all tensor ops using the core tinygrad ops (independent of frontend syntax)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Practice: Express the below in \"tinygrad ops\":\n",
    "- Broadcasting\n",
    "- Matmul (note: [matmul intuition](https://pytorch.org/blog/inside-the-matrix/))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different \"frontends\"\n",
    "\n",
    "- Classic numpy syntax (**only syntax allowed in exam**)\n",
    "- [Einsum](https://rockt.ai/2018/04/30/einsum) (index-centric ops in NumPy/Torch)\n",
    "- [Dumpy](https://dynomight.net/dumpy/) (loop-based NumPy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"Case studies\"\n",
    "\n",
    "- [AlexNet (2012) code](https://github.com/computerhistory/AlexNet-Source-Code): Python & custom binds to self-implemented CUDA kernels\n",
    "- [karpathy/micrograd](https://github.com/karpathy/micrograd): Educational tensor abstraction in Python's stdlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happens in torch's backend when instantiating a tensor?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "x = torch.tensor([1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Torch source code](https://github.com/pytorch/pytorch):\n",
    "1. `import torch` calls `torch/__init__.py`: brings functions into namespace (incl. C-bindings via `from torch._C import *`)\n",
    "2. Create instance via `torch/_tensor.py` ([link](https://github.com/pytorch/pytorch/blob/31d12b3955363b1dd45d2938f9cca08436c43387/torch/_tensor.py#L102)) ‚Äì> inherits from C base (`torch._C.TensorBase`) + Python with C-bindings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **üí° Torch: complex bindings to fast lower-level code & clean interface via Python (e.g., dunders, see later lectures)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compilation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "- Python interpreter: traverses code line by line **during runtime**\n",
    "- Calling C bindings as-we-go: \"eager mode\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compilation/\"lazy\" mode: adaptively mapping **larger** codeblocks into single kernels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: [JAX](https://kidger.site/thoughts/torch2jax/)\n",
    "\n",
    "JIT (just-in-time) compilation:\n",
    "\n",
    "- Decorate function with `@jax.jit`\n",
    "- First call: Python traces the ops\n",
    "- After tracing: compile a kernel and run it"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acceleration \"without Python\"\n",
    "- [Mojo](https://www.modular.com/mojo)\n",
    "  - Pro: everything written in one language (incl. compiler for any GPU vendor)\n",
    "  - Con: less mature; less adoption\n",
    "- [Julia](https://kidger.site/thoughts/jax-vs-julia/)\n",
    "  - Pro: more expressive than Python, incl. GPU backend\n",
    "  - Con: less adoption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goal: map some logic (e.g., math, slow Python code) to tensor ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task A (make slow Python code fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow(x: np.ndarray):\n",
    "    d1, d2 = x.shape\n",
    "    r = np.zeros((2, d1))\n",
    "    for i in range(d1):\n",
    "        for j in range(d2):\n",
    "            for k in range(2):\n",
    "                r[k, i] += (x[k, j] - x[i, j]) ** 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare testing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 42\n",
    "rng = np.random.default_rng(seed)\n",
    "x = rng.random((3, 7))  # d1 must be ‚â•2\n",
    "slow_result = slow(x)\n",
    "slow_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **‚ö†Ô∏è There are many ways to approach such tasks. Find the one that is suitable to \\*you\\*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simplify problem\n",
    "\n",
    "Examples:\n",
    "\n",
    "- Focus on subset of ops\n",
    "- Focus on computation (RHS of `=`) not where data is saved (LHS of `=`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loop visualizations\n",
    "*See lecture recording.*\n",
    "\n",
    "1. Draw each array as a 2D grid.\n",
    "2. Place axis arrows labeled by indices (e.g., vertical arrow `i` for `x` since `x[i,j]`).\n",
    "\n",
    "**Heuristics:**\n",
    "- Two arrows on same input axis ‚Üí broadcasting\n",
    "- Arrow on input but no corresponding arrow on output ‚Üí axis reduction\n",
    "- Arrow repositioned ‚Üí broadcasting (e.g., `v[None]-v` moves first axis of `v` to second dimension) or transposition\n",
    "- Output arrow spans fewer units than the original axis (e.g., 2 < d1) ‚Üí slicing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Shape matching\"\n",
    "How to get from input shape (`d1 x d2`) to output shape (`2 x d1`)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform `x` to `t1` and `t2`:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "t1      (3d array): 1 x d1 x d2  | Add singleton dim\n",
    "t2      (3d array): 2 x 1  x d2  | Slice first dim & add singleton dim\n",
    "result  (2d array): 2 x d1       | Reduce over d2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Formalization\n",
    "\n",
    "$$\n",
    "r_{k,i} = \\sum_{j=0}^{d_2-1} (x_{k,j} - x_{i,j})^2,\n",
    "$$\n",
    "where:\n",
    "$x \\in \\mathbb{R}^{d1 \\times d2}, \\quad r \\in \\mathbb{R}^{2 \\times d1}, \\quad k \\in \\{0, 1\\}, \\quad i \\in \\{0, 1, \\ldots, d_1-1\\}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solutions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow(x):  # Copy function for reference\n",
    "    d1, d2 = x.shape\n",
    "    r = np.zeros((2, d1))\n",
    "    for i in range(d1):\n",
    "        for j in range(d2):\n",
    "            for k in range(2):\n",
    "                r[k, i] += (x[k, j] - x[i, j]) ** 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fast(x):\n",
    "    return ((x[:2, None] - x) ** 2).sum(2)  #¬†Identical in torch\n",
    "\n",
    "np.allclose(slow(x), fast(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inefficient(x):\n",
    "    tmp = (x[:2] - x[:, None])\n",
    "    tmp = ((tmp) ** 2).sum(2)\n",
    "    result = tmp.transpose(1, 0)  # equivalent to tmp.T\n",
    "    return result\n",
    "\n",
    "np.allclose(slow(x), inefficient(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "t1      (3d array): 1  x 2 x d2 \n",
    "t2      (3d array): d1 x 1 x d2  | Slicing here would be more efficient\n",
    "result  (2d array): d1 x 2       | reduced over d2 (before transposition)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task B (make slow Python code fast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow(x: torch.Tensor):\n",
    "    d1, d2 = x.shape\n",
    "    assert d1 > 1 and d2 > 1  # This may not be stated in exam\n",
    "    r = torch.empty((d2, d1))  # `torch.zeros` may be more robust in practice if we accidentally don't overwrite all values\n",
    "    for i in range(d1):\n",
    "        for j in range(d2):\n",
    "            r[j, i] = (i != j) * x[i, j] ** 2\n",
    "    return r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "x = torch.rand(5, 7)\n",
    "slow_result = slow(x)\n",
    "slow_result.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Definition of $r$:\n",
    "$$\n",
    "r_{j, i} =\n",
    "\\begin{cases}\n",
    "x_{i, j}^2 & \\text{if } i \\neq j \\\\\n",
    "0 & \\text{if } i = j\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pdb import set_trace\n",
    "# Place `set_trace()` in code to debug\n",
    "\n",
    "\n",
    "def fast(x):\n",
    "    idx = torch.arange(min(x.shape))\n",
    "    x[idx, idx] = 0  # in-place, not as in `slow`: whether this is helpful depends on context\n",
    "    return (x ** 2).transpose(0, 1)\n",
    "\n",
    "\n",
    "# torch.all(slow(x) == fast(x))\n",
    "torch.allclose(slow(x), fast(x))  # account for numerical errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- After this lecture: know how to map formulas to torch/numpy\n",
    "- Conceptual/ML part (what is attention?) **not** yet required / covered by **future** lecture\n",
    "  - Lecture video: uses [this tool](https://poloclub.github.io/transformer-explainer/) to introduce **optional** context (e.g., for students already familiar with attention)\n",
    "\n",
    "Future: merge some core numpy/torch lecture content with applications lectures, e.g., do **full** conceptual discussion of attention **before** discussing the respective \"tensor mechanics\"? Feel free to provide feedback whether this would be more interesting (closer to applications) or overwhelming (more complexity)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formalizing attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shapes:\n",
    "- $B$: batch size\n",
    "- $T$: sequence (of tokens)\n",
    "- $N$: (attention) heads\n",
    "- $D$: embed dim (for one head)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input tensor (e.g., a ChatGPT user query embedded in a neural net feature space):\n",
    "$$\n",
    "X \\in \\mathbb{R}^{B \\times T \\times ND}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attain tensors below by multiplying $X$ with weight matrices & adjusting shape:\n",
    "\n",
    "$$Q,K,V\\in\\mathbb{R}^{B\\times N\\times T\\times D}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define mask (will mask out parts of the attention matrix later):\n",
    "$$\n",
    "M\\in\\{0,-\\infty\\}^{B\\times N\\times T\\times T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention matrix (two different notations; the latter notation only captures some dimensions):\n",
    "$$\n",
    "S=QK^\\top, \\qquad S_{ij}=\\sum_{k=1}^D Q_{ik}K_{jk}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize attention matrix (divide by embed dim size) & apply mask:\n",
    "$$\n",
    "\\tilde S=\\frac{S+M}{\\sqrt{D}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply numerically stable softmax:\n",
    "- [Softmax function](https://en.wikipedia.org/wiki/Softmax_function): create probability distribution\n",
    "- Shift-invariance ‚Üí shift to smaller values to avoid numerical problems (row-wise)\n",
    "- $S+M$: if $S$ is finite & $M$ is $-\\infty$ ‚Üí zero probability mass under softmax\n",
    "$$\n",
    "P_{ij}=\\frac{e^{\\tilde S_{ij}-m_i}}{\\sum_{j'} e^{\\tilde S_{ij'}-m_i}}, \\qquad m_i=\\max_j \\tilde S_{ij}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Matmul: attention matrix & $V$ (two different notations):\n",
    "$$\n",
    "O=PV, \\qquad O_{i:}=\\sum_j P_{ij}V_{j:}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding simplified attention\n",
    "\n",
    "- Set $B=1$ and $N=1$\n",
    "- Assume $Q$, $K$, and $V$ are given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def attention_core(Q, K, V, mask):\n",
    "    \"\"\"\n",
    "    Q,K,V: (T, D)\n",
    "    mask: (T, T) with 0 or -inf\n",
    "    returns O: (T, D)\n",
    "    \"\"\"\n",
    "    T, D = Q.shape\n",
    "    S = (Q @ K.T) * (1.0 / math.sqrt(D))   # (T, T)\n",
    "    S = S + mask\n",
    "    # manual softmax; F.softmax(S, dim=-1) from `from torch.nn import functional as F` is equivalent\n",
    "    m = S.max(dim=-1, keepdim=True).values\n",
    "    P = (S - m).exp()\n",
    "    P = P / P.sum(dim=-1, keepdim=True)\n",
    "    O = P @ V                               # (T, D)\n",
    "    return O\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coding full attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def attention_full(x, Wq, Wk, Wv, M, N):\n",
    "    \"\"\"\n",
    "    x:  (B, T, N*D)\n",
    "    Wq,Wk,Wv: (N*D, N*D)\n",
    "    M:  (B, N, T, T) with 0 or -inf\n",
    "    returns y: (B, T, N*D)\n",
    "    \"\"\"\n",
    "    B, T, C = x.shape  # C = N*D\n",
    "    assert C % N == 0\n",
    "    D = C // N\n",
    "\n",
    "    # \"neural net projection\"\n",
    "    q = x @ Wq                      # (B,T,N*D)\n",
    "    k = x @ Wk                      # (B,T,N*D)\n",
    "    v = x @ Wv                      # (B,T,N*D)\n",
    "\n",
    "    q = q.view(B, T, N, D).transpose(1, 2).contiguous()  # (B,N,T,D)\n",
    "    k = k.view(B, T, N, D).transpose(1, 2).contiguous()  # (B,N,T,D)\n",
    "    v = v.view(B, T, N, D).transpose(1, 2).contiguous()  # (B,N,T,D)\n",
    "\n",
    "    S = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(D))  # (B,N,T,T)\n",
    "    S = S + M\n",
    "\n",
    "    # manual softmax; F.softmax(S, dim=-1) is equivalent\n",
    "    m = S.max(dim=-1, keepdim=True).values\n",
    "    P = (S - m).exp()\n",
    "    P = P / P.sum(dim=-1, keepdim=True)\n",
    "\n",
    "    y = P @ v                                            # (B,N,T,D)\n",
    "    y = y.transpose(1, 2).contiguous().view(B, T, C)     # (B,T,N*D)\n",
    "\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"In the wild\":\n",
    "[karpathy/nanoGPT](https://github.com/karpathy/nanoGPT/blob/93a43d9a5c22450bbf06e78da2cb6eeef084b717/model.py#L29)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow-up: [karpathy/nanochat](karpathy/nanochat)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
